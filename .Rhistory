auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val,4), "\n")
# Precision-Recall Barplot
ggplot(data.frame(prob=prob, stroke=testData$stroke), aes(x=prob, fill=stroke)) +
geom_histogram(position="identity", bins=30, alpha=0.5) +
ggtitle("Predicted Probabilities – Logistic Regression") +
xlab("Predicted Probability") + ylab("Count") +
theme_minimal()
library(smotefamily)
library(caret)
library(pROC)
library(ggplot2)
# Load dataset
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicit selected features
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Train-test split (stratified)
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p=0.8, list=FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Categorical columns
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Convert categorical → numeric for SMOTE
train_num <- trainData
for (col in categorical_cols) {
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 4
)
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Logistic Regression Model
log_model <- glm(stroke ~ ., data=train_smote, family=binomial)
summary(log_model)
# Prepare test data: match factor levels
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = levels(as.factor(trainData[[col]])))
)
}
# Predict probabilities → classes
prob <- predict(log_model, newdata=testData, type="response")
pred_log <- ifelse(prob >= 0.5, "1", "0")
pred_log <- factor(pred_log, levels = c("0","1"))
# Make sure test labels are factors with same levels
testData$stroke <- factor(testData$stroke, levels = c("0","1"))
# Confusion Matrix (should now work)
cm_log <- confusionMatrix(pred_log, testData$stroke, positive = "1")
print(cm_log)
# Metrics
cat("Accuracy :", round(cm_log$overall['Accuracy'],4), "\n")
cat("Precision:", round(cm_log$byClass['Precision'],4), "\n")
cat("Recall   :", round(cm_log$byClass['Recall'],4), "\n")
cat("F1 Score :", round(cm_log$byClass['F1'],4), "\n")
#  Confusion Matrix Plot
cm_table <- as.data.frame(cm_log$table)
# Normalize by actual class (column-wise)
cm_table$Prop <- cm_table$Freq / tapply(cm_table$Freq, cm_table$Reference, sum)[cm_table$Reference]
ggplot(cm_table, aes(x=Prediction, y=Reference, fill=Prop)) +
geom_tile(color="white") +
geom_text(aes(label=sprintf("%.2f", Prop)), size=6, fontface="bold") +
scale_fill_gradient(low="#F5F5F5", high="#388E3C") +
labs(
title="Normalized Confusion Matrix – Logistic Regression",
subtitle="Shows column-wise proportion (useful for imbalanced data)",
x="Predicted Class",
y="Actual Class"
) +
theme_minimal()
# ROC Curve
roc_obj <- roc(testData$stroke, prob)
plot(roc_obj, main="ROC Curve – Logistic Regression", col="blue", lwd=3)
auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val,4), "\n")
# Precision-Recall Barplot
ggplot(data.frame(prob=prob, stroke=testData$stroke), aes(x=prob, fill=stroke)) +
geom_histogram(position="identity", bins=30, alpha=0.5) +
ggtitle("Predicted Probabilities – Logistic Regression") +
xlab("Predicted Probability") + ylab("Count") +
theme_minimal()
library(smotefamily)
library(caret)
library(pROC)
library(ggplot2)
# Load dataset
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicit selected features
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Train-test split (stratified)
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p=0.8, list=FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Categorical columns
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Convert categorical → numeric for SMOTE
train_num <- trainData
for (col in categorical_cols) {
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 4
)
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Logistic Regression Model
log_model <- glm(stroke ~ ., data=train_smote, family=binomial)
summary(log_model)
# Prepare test data: match factor levels
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = levels(as.factor(trainData[[col]])))
)
}
# Predict probabilities → classes
prob <- predict(log_model, newdata=testData, type="response")
pred_log <- ifelse(prob >= 0.5, "1", "0")
pred_log <- factor(pred_log, levels = c("0","1"))
# Make sure test labels are factors with same levels
testData$stroke <- factor(testData$stroke, levels = c("0","1"))
# Confusion Matrix (should now work)
cm_log <- confusionMatrix(pred_log, testData$stroke, positive = "1")
print(cm_log)
# Metrics
cat("Accuracy :", round(cm_log$overall['Accuracy'],4), "\n")
cat("Precision:", round(cm_log$byClass['Precision'],4), "\n")
cat("Recall   :", round(cm_log$byClass['Recall'],4), "\n")
cat("F1 Score :", round(cm_log$byClass['F1'],4), "\n")
cm_df <- as.data.frame(cm_log$table)
# Normalize by row (so color gradient works properly)
cm_df$Prop <- cm_df$Freq / rowSums(cm_log$table)[cm_df$Prediction]
# Plot
ggplot(cm_df, aes(x=Prediction, y=Reference, fill=Prop)) +
geom_tile(color="white") +
geom_text(aes(label=sprintf("%.2f", Prop)), size=6, fontface="bold") +
scale_fill_gradient(low="#F5F5F5", high="#388E3C") +
labs(
title="Normalized Confusion Matrix – Logistic Regression",
subtitle="Shows proportion per predicted class",
x="Predicted Class",
y="Actual Class"
) +
theme_minimal()
# ROC Curve
roc_obj <- roc(testData$stroke, prob)
plot(roc_obj, main="ROC Curve – Logistic Regression", col="blue", lwd=3)
auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val,4), "\n")
# Precision-Recall Barplot
ggplot(data.frame(prob=prob, stroke=testData$stroke), aes(x=prob, fill=stroke)) +
geom_histogram(position="identity", bins=30, alpha=0.5) +
ggtitle("Predicted Probabilities – Logistic Regression") +
xlab("Predicted Probability") + ylab("Count") +
theme_minimal()
library(smotefamily)
library(caret)
library(pROC)
library(ggplot2)
# Load dataset
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicit selected features
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Train-test split (stratified)
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p=0.8, list=FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Categorical columns
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Convert categorical → numeric for SMOTE
train_num <- trainData
for (col in categorical_cols) {
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 4
)
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Logistic Regression Model
log_model <- glm(stroke ~ ., data=train_smote, family=binomial)
summary(log_model)
# Prepare test data: match factor levels
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = levels(as.factor(trainData[[col]])))
)
}
# Predict probabilities → classes
prob <- predict(log_model, newdata=testData, type="response")
pred_log <- ifelse(prob >= 0.5, "1", "0")
pred_log <- factor(pred_log, levels = c("0","1"))
# Make sure test labels are factors with same levels
testData$stroke <- factor(testData$stroke, levels = c("0","1"))
# Confusion Matrix (should now work)
cm_log <- confusionMatrix(pred_log, testData$stroke, positive = "1")
print(cm_log)
# Metrics
cat("Accuracy :", round(cm_log$overall['Accuracy'],4), "\n")
cat("Precision:", round(cm_log$byClass['Precision'],4), "\n")
cat("Recall   :", round(cm_log$byClass['Recall'],4), "\n")
cat("F1 Score :", round(cm_log$byClass['F1'],4), "\n")
# ROC Curve
roc_obj <- roc(testData$stroke, prob)
plot(roc_obj, main="ROC Curve – Logistic Regression", col="blue", lwd=3)
auc_val <- auc(roc_obj)
cat("AUC:", round(auc_val,4), "\n")
# Precision-Recall Barplot
ggplot(data.frame(prob=prob, stroke=testData$stroke), aes(x=prob, fill=stroke)) +
geom_histogram(position="identity", bins=30, alpha=0.5) +
ggtitle("Predicted Probabilities – Logistic Regression") +
xlab("Predicted Probability") + ylab("Count") +
theme_minimal()
###############################################################################
#                          1st Model – Decision Tree
###############################################################################
library(rpart)
library(rpart.plot)
library(smotefamily)
library(caret)
library(ggplot2)
# Load dataset
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicit feature list (prevents mismatch bugs)
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Stratified 80/20 split
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Categorical columns
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Prepare numeric data for SMOTE
train_num <- trainData
for (col in categorical_cols) {
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
# Target to numeric for SMOTE
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 3
)
# Rebuild SMOTE-balanced dataset
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Cost-sensitive loss matrix (penalize FN heavily)
loss_matrix <- matrix(
c(0, 1,
10, 0),
nrow = 2,
byrow = TRUE
)
colnames(loss_matrix) <- rownames(loss_matrix) <- levels(train_smote$stroke)
# Train Decision Tree
tree_model <- rpart(
stroke ~ .,
data = train_smote,
method = "class",
parms = list(loss = loss_matrix),
control = rpart.control(
cp = 0.001,
minsplit = 50,
minbucket = 10,
maxdepth = 10
)
)
# Plot tree
rpart.plot(
tree_model,
type = 3,
extra = 101,
fallen.leaves = TRUE,
main = "Decision Tree for Stroke Prediction"
)
# Encode test data using training levels (CRITICAL FIX)
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]],
levels = levels(as.factor(trainData[[col]])))
)
}
# Predictions
pred_dt <- predict(tree_model, newdata = testData, type = "class")
# Evaluation
cm_dt <- confusionMatrix(pred_dt, testData$stroke, positive = "1")
print(cm_dt)
cm_df <- as.data.frame(cm_dt$table)
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 6, fontface = "bold") +
scale_fill_gradient(low = "#F5F5F5", high = "#1976D2") +
labs(
title = "Confusion Matrix – Decision Tree",
subtitle = "Cost-Sensitive Learning with SMOTE",
x = "Predicted Class",
y = "Actual Class"
) +
theme_minimal()
# Metrics
cat("Accuracy :", round(cm_dt$overall["Accuracy"], 4), "\n")
cat("Precision:", round(cm_dt$byClass["Precision"], 4), "\n")
cat("Recall   :", round(cm_dt$byClass["Recall"], 4), "\n")
cat("F1 Score :", round(cm_dt$byClass["F1"], 4), "\n")
###############################################################################
#                         2nd Model Random Forest
###############################################################################
library(randomForest)
library(smotefamily)
library(caret)
# Load dataset
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicitly define selected features (avoid mismatch)
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Train-test split (80/20)
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Categorical columns (ONLY from selected features)
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Copy training data for SMOTE
train_num <- trainData
# Encode categorical variables numerically for SMOTE
for (col in categorical_cols) {
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
# Convert target to numeric (required by SMOTE)
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 3
)
# Reconstruct SMOTE dataset
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Train Random Forest
set.seed(123)
rf_model <- randomForest(
stroke ~ .,
data = train_smote,
ntree = 300,
mtry = floor(sqrt(ncol(train_smote) - 1)),
importance = TRUE,
classwt = c("0" = 1, "1" = 8)
)
print(rf_model)
# Encode test set using training levels (CRITICAL FIX)
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]],
levels = levels(as.factor(trainData[[col]])))
)
}
# Predict probabilities
probs <- predict(rf_model, newdata = testData, type = "prob")[, 2]
# Lower threshold to boost recall
pred_rf <- ifelse(probs > 0.3, "1", "0")
pred_rf <- factor(pred_rf, levels = c("0", "1"))
# Evaluation
cm_rf <- confusionMatrix(pred_rf, testData$stroke, positive = "1")
print(cm_rf)
cm_df <- as.data.frame(cm_rf$table)
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 6, fontface = "bold") +
scale_fill_gradient(low = "#F5F5F5", high = "#D32F2F") +
labs(
title = "Confusion Matrix – Random Forest",
subtitle = "Threshold = 0.3 (Recall-Optimized)",
x = "Predicted Label",
y = "Actual Label"
) +
theme_minimal()
# Metrics
cat("Accuracy :", round(cm_rf$overall["Accuracy"], 4), "\n")
cat("Precision:", round(cm_rf$byClass["Precision"], 4), "\n")
cat("Recall   :", round(cm_rf$byClass["Recall"], 4), "\n")
cat("F1 Score :", round(cm_rf$byClass["F1"], 4), "\n")
# Feature importance
varImpPlot(rf_model, main = "Random Forest Feature Importance")
###############################################################################
# Analysis / Conclusion
###############################################################################
cat("\nAnalysis:\n")
cat("The Random Forest model demonstrates strong recall performance after\n")
cat("feature selection, SMOTE balancing, class weighting, and threshold tuning.\n")
cat("This configuration prioritizes sensitivity, which is appropriate for\n")
cat("stroke prediction where false negatives are costly.\n")
