col=c("blue", "green", "red"), lty=1)
selected_features <- readRDS("~/Desktop/stroke-dataset-analysis-in-r/selected_features.rds")
selected_features <- readRDS("~/Desktop/stroke-dataset-analysis-in-r/selected_features.rds")
###############################################################################
#                                 Data Cleaning
###############################################################################
library("DescTools")
###############################################################################
#                                 Data Cleaning
###############################################################################
library("DescTools")
install DescTools
install.packages("DescTools")
###############################################################################
#                                 Data Cleaning
###############################################################################
library("DescTools")
df <- read.csv("stroke_data.csv", na.strings = c("N/A", "Unknown"))
df.row <- df
# remove ids col
df <- df[,-1]
# remove other gender
df <- df[df$gender != 'Other',]
df
summary(df)
str(df)
dim(df)
# The data consists of 5109 entries and 12 columns.
# First part of data cleaning: check for incompleteness (missing values)
# check for null values in each column
for(col in colnames(df)){
print(col)
print(any(is.na(df[,col])))
}
# All columns do not have any null values except BMI and smoking status columns.
dim(df[is.na(df$bmi),])
# The dataframe has 201 rows in which the BMI is null.
dim(df[is.na(df$smoking_status),])
# The dataframe has 1544 rows in which the smoking status is null.
# Second part of data cleaning: check for inconsistencies (contradictions)
for(col in c('gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status')){
print(col)
print(unique(df[,col]))
}
any(df$work_type == "children" & df$age >= 18)
# All children are indeed below 18.
any(df$work_type == "children" & df$ever_married == "Yes")
# All the values for "ever_married" column for children has the value "No."
df[df$work_type == "children" & !is.na(df$smoking_status) & !df$smoking_status == "never smoked", ]
# These values are inconsistent assuming that children under 18 do not smoke (or formerly smoked).
# Decision: Set the smoke_status for all children to "never smoked" in the dataset.
df$smoking_status[df$work_type == "children" & df$smoking_status != "never smoked"] <- "never smoked"
# Decision: Impute missing non-children smoking_status as the mode (never_smoked)
df$smoking_status[is.na(df$smoking_status)] <- Mode(df$smoking_status, na.rm = TRUE)
# Third part of data cleaning: remove duplicate rows
sum(duplicated(df))
# No duplicated rows.
# Fourth part of data cleaning: detect incorrect data (check for outliers)
q3_glucose = quantile(df$avg_glucose_level, 0.75)
q1_glucose = quantile(df$avg_glucose_level, 0.25)
iqr_glucose = q3_glucose - q1_glucose
q3_bmi = quantile(df$bmi, 0.75, na.rm = TRUE)
q1_bmi = quantile(df$bmi, 0.25, na.rm = TRUE)
iqr_bmi = q3_bmi - q1_bmi
par(mfcol = c(1, 3))
boxplot(df["age"], xlab = "Age")
boxplot(df["avg_glucose_level"], xlab = "Average Glucose Level")
abline(h = q3_glucose + (3 * iqr_glucose), col = "red", lty = 2, lwd = 2)
boxplot(df["bmi"], xlab = "BMI")
abline(h = q3_bmi + (3 * iqr_bmi), col = "red", lty = 2, lwd = 2)
any(df$age < 1)
# Age doesn't seem to have an outlier but the data has around 43 rows of age less than 1.
# There are many outliers for avg_glucose_level and bmi, so we define that definitely certain outliers are above the outer fence (more than Q3 by 3 IQR or less than Q1 by 3 IQR)
# Decision: REMOVE definite outliers
df$avg_glucose_level[df$avg_glucose_level > q3_glucose + (3 * iqr_glucose) | df$avg_glucose_level < q1_glucose - (3 * iqr_glucose)] <- NA
df$bmi[df$bmi > q3_bmi + (3 * iqr_bmi) | df$bmi < q1_bmi - (3 * iqr_bmi)] <- NA
# Outliers between the outer fence and inner fence are suspected outliers as mentioned in the lecture. They will be experimented with during model implementation.
# Decision: KEEP suspected outliers.
# Revisualize
par(mfcol = c(1, 3))
boxplot(df["age"], xlab = "Age")
boxplot(df["avg_glucose_level"], xlab = "Average Glucose Level")
abline(h = q3_glucose + (3 * iqr_glucose), col = "red", lty = 2, lwd = 2)
boxplot(df["bmi"], xlab = "BMI")
abline(h = q3_bmi + (3 * iqr_bmi), col = "red", lty = 2, lwd = 2)
# Decision: Impute avg_glucose_level and bmi using mean.
df$avg_glucose_level[is.na(df$avg_glucose_level)] <- mean(df$avg_glucose_level, na.rm = TRUE)
df$bmi[is.na(df$bmi)] <- mean(df$bmi, na.rm = TRUE)
write.csv(df, "cleaned_stroke_data.csv", row.names=FALSE)
###############################################################################
#                         Exploratory Data Analysis (EDA)
###############################################################################
df <- read.csv("cleaned_stroke_data.csv", na.strings = c("N/A", "Unknown"))
numeric_cols <- c("age", "avg_glucose_level", "bmi", "stroke")
df_numeric <- df[, numeric_cols]
# Create directory if it does not exist
if(!dir.exists("plots")) dir.create("plots")
# Pastel colors for all barplots
pastel_colors <- c("#FFB3BA", "#BAE1FF", "#BAFFC9", "#FFFFBA", "#FFDFBA")
png("plots/pairwise_numeric_vars.png", width=1000, height=1000)
# Pairwise plot for numeric variables and target variable
pairs(df_numeric,
main = "Pairwise Plots of Numeric Variables",
pch = 21,
bg = c("red", "blue")[as.factor(df$stroke)]) # color by stroke
# Conclusion: Stroke is proportional to age
# Conclusion: High BMI is tied to higher ages (Higher BMI -> Most likely high age)
dev.off()
png("plots/boxplots_numeric_vs_stroke.png", width=1000, height=1000)
par(mfrow=c(1,3))
boxplot(age ~ stroke, data=df, main="Age vs Stroke")
# Conclusion: Stroke is CONFIRMED to be proportional to age
boxplot(avg_glucose_level ~ stroke, data=df, main="Avg Glucose vs Stroke")
# Conclusion: The average glucose levels are generally higher for patients with a stroke
boxplot(bmi ~ stroke, data=df, main="BMI vs Stroke")
# Conclusion: Stroke seems to increase for intermediate values of BMI (not too high or too low)
dev.off()
png("plots/barplots_gender_hypertension_heartdisease.png", width=1000, height=1000)
par(mfrow=c(1,3))
table(df$gender, df$stroke)
barplot(table(df$gender, df$stroke), beside=TRUE, legend=TRUE, main="Gender vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:2])
# Conclusion: Females may seem to have more strokes but in reality the hypothesis testing shows that there is no correleation
# This may happen due to data imbalance (more females than males)
table(df$hypertension, df$stroke)
barplot(table(df$hypertension, df$stroke), beside=TRUE, legend=TRUE,
main="Hypertension vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[3:4])
# Conclusion: Patients with hypertension are more likely to have a stroke compared to patients with no hypertension
table(df$heart_disease, df$stroke)
barplot(table(df$heart_disease, df$stroke), beside=TRUE, legend=TRUE,
main="Heart Disease vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[5:1])
# Conclusion: Patients with heart disease are more likely to have a stroke compared to patients with no heart disease
dev.off()
png("plots/barplots_evermarried_worktype_residencetype_smoking.png", width=1000, height=1000)
par(mfrow=c(2, 2))
# Ever Married vs Stroke
table(df$ever_married, df$stroke)
barplot(table(df$ever_married, df$stroke), beside=TRUE, legend=TRUE,
main="Ever Married vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:2])
# Conclusion: No apparent correlation
# Work Type vs Stroke
table(df$work_type, df$stroke)
barplot(table(df$work_type, df$stroke), beside=TRUE, legend=TRUE,
main="Work Type vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:5])
# Conclusion: No apparent correlation
# Residence Type vs Stroke
table(df$Residence_type, df$stroke)
barplot(table(df$Residence_type, df$stroke), beside=TRUE, legend=TRUE,
main="Residence Type vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[2:3])
# Conclusion: No correlation (backed by hypothesis testing)
# Smoking Status vs Stroke
table(df$smoking_status, df$stroke)
barplot(table(df$smoking_status, df$stroke), beside=TRUE, legend=TRUE,
main="Smoking Status vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:5])
# Conclusion: No apparent correlation
dev.off()
png("plots/histograms_age_glucose_bmi.png", width=1000, height=1000)
par(mfrow=c(3, 1))
# Age Histogram
hist(df$age, breaks=20, col="skyblue", main="Age Distribution with Density", xlab="Age", freq=FALSE)
lines(density(df$age, na.rm=TRUE), col="red", lwd=2)
# Age seems to be trimodal (there is a mode at each age group)
# Conclusion: Age is trimodal and over-represented for the intermediate age group
# Average Glucose Level Histogram
hist(df$avg_glucose_level, breaks=20, col="lightgreen", main="Avg Glucose Level Distribution with Density", xlab="Avg Glucose Level", freq=FALSE)
lines(density(df$avg_glucose_level, na.rm=TRUE), col="red", lwd=2)
# Average glucose level seems to be bimodal
# Conclusion: We can assume it's bell-shaped but right skewed (needs log scaling)
# BMI Histogram
hist(df$bmi, breaks=20, col="orange", main="BMI Distribution with Density", xlab="BMI", freq=FALSE)
lines(density(df$bmi, na.rm=TRUE), col="red", lwd=2)
# Conclusion: BMI is almost normally distributed, slightly right skewed. May need log scaling
dev.off()
png("plots/pie_stroke.png", width=1000, height=1000)
par(mfrow=c(1,1))
# Stroke Pie Chart
stroke_counts <- table(df$stroke)
pct <- round(stroke_counts / sum(stroke_counts) * 100, 2)
lbls <- paste(c("No Stroke", "Stroke"), pct, sep=" ")
lbls <- paste(lbls, "%", sep="")
pie(stroke_counts, labels=lbls, main="Stroke vs No Stroke Distribution", col=c("lightgreen","red"))
# Conclusion: Data is HIGHLY unbalanced. We need to use SMOTE for data augmentation to balance the dataset for predictions
dev.off()
###############################################################################
#                         Exploratory Data Analysis (EDA)
###############################################################################
df <- read.csv("cleaned_stroke_data.csv", na.strings = c("N/A", "Unknown"))
numeric_cols <- c("age", "avg_glucose_level", "bmi", "stroke")
df_numeric <- df[, numeric_cols]
# Create directory if it does not exist
if(!dir.exists("plots")) dir.create("plots")
# Pastel colors for all barplots
pastel_colors <- c("#FFB3BA", "#BAE1FF", "#BAFFC9", "#FFFFBA", "#FFDFBA")
png("plots/pairwise_numeric_vars.png", width=1000, height=1000)
# Pairwise plot for numeric variables and target variable
pairs(df_numeric,
main = "Pairwise Plots of Numeric Variables",
pch = 21,
bg = c("red", "blue")[as.factor(df$stroke)]) # color by stroke
# Conclusion: Stroke is proportional to age
# Conclusion: High BMI is tied to higher ages (Higher BMI -> Most likely high age)
dev.off()
png("plots/boxplots_numeric_vs_stroke.png", width=1000, height=1000)
par(mfrow=c(1,3))
boxplot(age ~ stroke, data=df, main="Age vs Stroke")
# Conclusion: Stroke is CONFIRMED to be proportional to age
boxplot(avg_glucose_level ~ stroke, data=df, main="Avg Glucose vs Stroke")
# Conclusion: The average glucose levels are generally higher for patients with a stroke
boxplot(bmi ~ stroke, data=df, main="BMI vs Stroke")
# Conclusion: Stroke seems to increase for intermediate values of BMI (not too high or too low)
dev.off()
png("plots/barplots_gender_hypertension_heartdisease.png", width=1000, height=1000)
par(mfrow=c(1,3))
table(df$gender, df$stroke)
barplot(table(df$gender, df$stroke), beside=TRUE, legend=TRUE, main="Gender vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:2])
# Conclusion: Females may seem to have more strokes but in reality the hypothesis testing shows that there is no correleation
# This may happen due to data imbalance (more females than males)
table(df$hypertension, df$stroke)
barplot(table(df$hypertension, df$stroke), beside=TRUE, legend=TRUE,
main="Hypertension vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[3:4])
# Conclusion: Patients with hypertension are more likely to have a stroke compared to patients with no hypertension
table(df$heart_disease, df$stroke)
barplot(table(df$heart_disease, df$stroke), beside=TRUE, legend=TRUE,
main="Heart Disease vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[5:1])
# Conclusion: Patients with heart disease are more likely to have a stroke compared to patients with no heart disease
dev.off()
png("plots/barplots_evermarried_worktype_residencetype_smoking.png", width=1000, height=1000)
par(mfrow=c(2, 2))
# Ever Married vs Stroke
table(df$ever_married, df$stroke)
barplot(table(df$ever_married, df$stroke), beside=TRUE, legend=TRUE,
main="Ever Married vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:2])
# Conclusion: No apparent correlation
# Work Type vs Stroke
table(df$work_type, df$stroke)
barplot(table(df$work_type, df$stroke), beside=TRUE, legend=TRUE,
main="Work Type vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:5])
# Conclusion: No apparent correlation
# Residence Type vs Stroke
table(df$Residence_type, df$stroke)
barplot(table(df$Residence_type, df$stroke), beside=TRUE, legend=TRUE,
main="Residence Type vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[2:3])
# Conclusion: No correlation (backed by hypothesis testing)
# Smoking Status vs Stroke
table(df$smoking_status, df$stroke)
barplot(table(df$smoking_status, df$stroke), beside=TRUE, legend=TRUE,
main="Smoking Status vs Stroke", xlab="Stroke", ylab="Count", col=pastel_colors[1:5])
# Conclusion: No apparent correlation
dev.off()
png("plots/histograms_age_glucose_bmi.png", width=1000, height=1000)
par(mfrow=c(3, 1))
# Age Histogram
hist(df$age, breaks=20, col="skyblue", main="Age Distribution with Density", xlab="Age", freq=FALSE)
lines(density(df$age, na.rm=TRUE), col="red", lwd=2)
# Age seems to be trimodal (there is a mode at each age group)
# Conclusion: Age is trimodal and over-represented for the intermediate age group
# Average Glucose Level Histogram
hist(df$avg_glucose_level, breaks=20, col="lightgreen", main="Avg Glucose Level Distribution with Density", xlab="Avg Glucose Level", freq=FALSE)
lines(density(df$avg_glucose_level, na.rm=TRUE), col="red", lwd=2)
# Average glucose level seems to be bimodal
# Conclusion: We can assume it's bell-shaped but right skewed (needs log scaling)
# BMI Histogram
hist(df$bmi, breaks=20, col="orange", main="BMI Distribution with Density", xlab="BMI", freq=FALSE)
lines(density(df$bmi, na.rm=TRUE), col="red", lwd=2)
# Conclusion: BMI is almost normally distributed, slightly right skewed. May need log scaling
dev.off()
png("plots/pie_stroke.png", width=1000, height=1000)
par(mfrow=c(1,1))
# Stroke Pie Chart
stroke_counts <- table(df$stroke)
pct <- round(stroke_counts / sum(stroke_counts) * 100, 2)
lbls <- paste(c("No Stroke", "Stroke"), pct, sep=" ")
lbls <- paste(lbls, "%", sep="")
pie(stroke_counts, labels=lbls, main="Stroke vs No Stroke Distribution", col=c("lightgreen","red"))
# Conclusion: Data is HIGHLY unbalanced. We need to use SMOTE for data augmentation to balance the dataset for predictions
dev.off()
###############################################################################
#                             Hypothesis Testing
###############################################################################
df <- read.csv("cleaned_stroke_data.csv", na.strings = c("N/A", "Unknown"))
# Prepare variables
df$stroke_group <- factor(df$stroke, levels = c(0,1), labels = c("no","yes"))
df$stroke_num   <- as.numeric(df$stroke)
df$gender         <- factor(df$gender)
df$hypertension   <- factor(df$hypertension)
df$heart_disease  <- factor(df$heart_disease)
df$ever_married   <- factor(df$ever_married)
df$work_type      <- factor(df$work_type)
df$Residence_type <- factor(df$Residence_type)
df$smoking_status <- factor(df$smoking_status)
# Helper functions
safe_t_test <- function(formula, data) {
print(t.test(formula, data = data))
}
safe_wilcox_test <- function(formula, data) {
print(wilcox.test(formula, data = data))
}
# Age: significant difference between stroke vs non-stroke groups
# p < 2.2e-16 (both t-test and Wilcoxon)
safe_t_test(age ~ stroke_group, data = df)
safe_wilcox_test(age ~ stroke_group, data = df)
# Average glucose level: significant difference
# t-test p ≈ 1.76e-08, Wilcoxon p ≈ 4.6e-07
safe_t_test(avg_glucose_level ~ stroke_group, data = df)
safe_wilcox_test(avg_glucose_level ~ stroke_group, data = df)
# BMI: small but significant difference
# t-test p ≈ 0.000185, Wilcoxon p ≈ 7e-05
safe_t_test(bmi ~ stroke_group, data = df)
safe_wilcox_test(bmi ~ stroke_group, data = df)
# Gender: NOT significant
# t-test p ≈ 0.52, Wilcoxon p ≈ 0.52
safe_t_test(stroke_num ~ gender, data = df)
safe_wilcox_test(stroke_num ~ gender, data = df)
# Hypertension: highly significant
safe_t_test(stroke_num ~ hypertension, data = df)
safe_wilcox_test(stroke_num ~ hypertension, data = df)
# Heart disease: highly significant
safe_t_test(stroke_num ~ heart_disease, data = df)
safe_wilcox_test(stroke_num ~ heart_disease, data = df)
# Ever married: significant difference
safe_t_test(stroke_num ~ ever_married, data = df)
safe_wilcox_test(stroke_num ~ ever_married, data = df)
# Residence type: NOT significant
# t-test p ≈ 0.27, Wilcoxon p ≈ 0.27
safe_t_test(stroke_num ~ Residence_type, data = df)
safe_wilcox_test(stroke_num ~ Residence_type, data = df)
# Work type: significant overall differences (ANOVA p ≈ 4.91e-10)
aov_work <- aov(stroke_num ~ work_type, data = df)
summary(aov_work)
TukeyHSD(aov_work)
# Smoking status: significant differences (ANOVA p ≈ 3.23e-06)
aov_smoke <- aov(stroke_num ~ smoking_status, data = df)
summary(aov_smoke)
TukeyHSD(aov_smoke)
# Non-significant predictors:
#   - gender (p > 0.05)
#   - Residence_type (p > 0.05)
#
# Decision: DROP both variables.
df$gender <- NULL
df$Residence_type <- NULL
write.csv(df, "data_after_testing.csv", row.names = FALSE)
###############################################################################
#                              Feature Selection
###############################################################################
# Load data after hypothesis testing
df <- read.csv("data_after_testing.csv")
# Feature evaluation table
feature_eval <- data.frame(
feature = character(),
p_value = numeric(),
test_type = character(),
decision = character(),
reason = character(),
stringsAsFactors = FALSE
)
# Fill with results from hypothesis testing:
feature_eval <- rbind(feature_eval,
data.frame(
feature = "age",
p_value = 2.2e-16,
test_type = "t-test",
decision = "KEEP",
reason = "Highly significant (p < 2.2e-16)"
),
data.frame(
feature = "gender",
p_value = 0.52,
test_type = "t-test",
decision = "DROP",
reason = "Not significant (p = 0.52)"
),
data.frame(
feature = "hypertension",
p_value = 3.6e-09,
test_type = "t-test & wilcoxon",
decision = "KEEP",
reason = "Highly significant, known medical risk factor"
),
data.frame(
feature = "heart_disease",
p_value = 4.5e-08,
test_type = "t-test & wilcoxon",
decision = "KEEP",
reason = "Highly significant, known medical risk factor"
),
data.frame(
feature = "ever_married",
p_value = 2.2e-16,
test_type = "t-test & wilcoxon",
decision = "KEEP",
reason = "Significant difference found"
),
data.frame(
feature = "work_type",
p_value = 4.91e-10,
test_type = "ANOVA",
decision = "KEEP",
reason = "Highly significant overall differences (ANOVA p ≈ 4.91e-10)"
),
data.frame(
feature = "Residence_type",
p_value = 0.27,
test_type = "t-test & wilcoxon",
decision = "DROP",
reason = "Not significant (p ≈ 0.27)"
),
data.frame(
feature = "smoking_status",
p_value = 3.23e-06,
test_type = "ANOVA",
decision = "KEEP",
reason = "Significant differences (ANOVA p ≈ 3.23e-06)"
),
data.frame(
feature = "avg_glucose_level",
p_value = 1.76e-08,  # t-test p-value
test_type = "t-test",
decision = "KEEP",
reason = "Highly significant (t-test p ≈ 1.76e-08)"
),
data.frame(
feature = "bmi",
p_value = 0.000185,  # t-test p-value
test_type = "t-test",
decision = "KEEP",
reason = "Significant (t-test p ≈ 0.000185)"
)
)
# Selected features (excluding the dropped ones)
selected_features <- feature_eval$feature[feature_eval$decision == "KEEP"]
# Remove duplicates and decision variable
selected_features <- unique(selected_features)
selected_features <- selected_features[!selected_features %in% "stroke"]
# Save for model
saveRDS(selected_features, "selected_features.rds")
# Save the full evaluation table
write.csv(feature_eval, "feature_selection_evaluation.csv", row.names = FALSE)
cat("Feature Selection Complete!
Based on hypothesis testing results, selected", length(selected_features), "features:
", paste(selected_features, collapse = ", "))
###############################################################################
#                          1st Model Decision Tree
###############################################################################
library(rpart)
library(rpart.plot)
library(smotefamily)
library(caret)
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Load selected features
selected_features <- readRDS("selected_features.rds")
selected_features <- c(selected_features, "stroke")
df <- df[, selected_features]
# Split 80/20
set.seed(123)
trainIndex <- sample(1:nrow(df), 0.8 * nrow(df))
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Identify categorical cols
categorical_cols <- intersect(
c("gender","hypertension","heart_disease","ever_married",
"work_type","Residence_type","smoking_status"),
selected_features
)
# Convert categorical → numeric for SMOTE
train_num <- trainData
for(col in categorical_cols){
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 3
)
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Tuned Hyperparameters
loss_matrix <- matrix(c(0, 1, 10, 0), nrow=2, byrow=TRUE)
colnames(loss_matrix) <- rownames(loss_matrix) <- levels(train_smote$stroke)
tree_model <- rpart(
stroke ~ .,
data = train_smote,
method = "class",
parms = list(loss = loss_matrix),
control = rpart.control(
cp = 0.001,
minsplit = 50,
minbucket = 10,
maxdepth = 10
)
)
rpart.plot(tree_model, type = 3, extra = 101, fallen.leaves = TRUE,
main = "Decision Tree")
# Prepare test data
for(col in categorical_cols){
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = unique(trainData[[col]]))
)
}
# Predictions & Evaluation
pred <- predict(tree_model, newdata = testData, type = "class")
cm <- confusionMatrix(pred, testData$stroke, positive = "1")
print(cm)
cat("Accuracy :", round(cm$overall['Accuracy'], 4), "\n")
cat("Precision:", round(cm$byClass['Precision'], 4), "\n")
cat("Recall   :", round(cm$byClass['Recall'], 4), "\n")
cat("F1 Score :", round(cm$byClass['F1'], 4), "\n")
