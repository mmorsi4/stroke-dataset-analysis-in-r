10, 0),
nrow = 2,
byrow = TRUE
)
colnames(loss_matrix) <- rownames(loss_matrix) <- levels(train_smote$stroke)
# Train Decision Tree
tree_model <- rpart(
stroke ~ .,
data = train_smote,
method = "class",
parms = list(loss = loss_matrix),
control = rpart.control(
cp = 0.001,
minsplit = 50,
minbucket = 10,
maxdepth = 10
)
)
# Plot tree
rpart.plot(
tree_model,
type = 3,
extra = 101,
fallen.leaves = TRUE,
main = "Decision Tree for Stroke Prediction"
)
# Encode test data using training levels (CRITICAL FIX)
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]],
levels = levels(as.factor(trainData[[col]])))
)
}
# Predictions
pred_dt <- predict(tree_model, newdata = testData, type = "class")
# Evaluation
cm_dt <- confusionMatrix(pred_dt, testData$stroke, positive = "1")
print(cm_dt)
cm_df <- as.data.frame(cm_dt$table)
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 6, fontface = "bold") +
scale_fill_gradient(low = "#F5F5F5", high = "#1976D2") +
labs(
title = "Confusion Matrix â€“ Decision Tree",
subtitle = "Cost-Sensitive Learning with SMOTE",
x = "Predicted Class",
y = "Actual Class"
) +
theme_minimal()
# Metrics
cat("Accuracy :", round(cm_dt$overall["Accuracy"], 4), "\n")
cat("Precision:", round(cm_dt$byClass["Precision"], 4), "\n")
cat("Recall   :", round(cm_dt$byClass["Recall"], 4), "\n")
cat("F1 Score :", round(cm_dt$byClass["F1"], 4), "\n")
library(caret)
library(smotefamily)
library(e1071)
library(tidyr)
library(dplyr)
library(ggplot2)
# ----------------- Load and Prepare Data -----------------
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicit feature list (prevents mismatch bugs)
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
set.seed(123)
trainIndex <- sample(1:nrow(df), 0.8 * nrow(df))
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
categorical_cols <- c("hypertension","heart_disease",
"ever_married","work_type","smoking_status")
# Convert categorical to numeric for SMOTE
train_num <- trainData
for(col in categorical_cols){
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num)=="stroke")],
target = train_num$stroke,
K = 5,
dup_size = 5
)
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Prepare test data
for(col in categorical_cols){
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = unique(trainData[[col]]))
)
}
# ----------------- SVM Model -----------------
svm_model <- svm(
stroke ~ .,
data = train_smote,
kernel = "radial",
class.weights = c("0" = 1, "1" = 3),
probability = TRUE
)
svm_pred <- predict(svm_model, newdata = testData)
svm_cm <- confusionMatrix(svm_pred, testData$stroke, positive = "1")
cat("\n--- Support Vector Machine ---\n")
cat("Accuracy :", round(svm_cm$overall["Accuracy"], 4), "\n")
cat("Precision:", round(svm_cm$byClass["Precision"], 4), "\n")
cat("Recall   :", round(svm_cm$byClass["Recall"], 4), "\n")
cat("F1 Score :", round(svm_cm$byClass["F1"], 4), "\n")
# Confusion Matrix Plot (normalized)
svm_cm_df <- as.data.frame(svm_cm$table)
svm_cm_df$Prop <- svm_cm_df$Freq / rowSums(svm_cm$table)[svm_cm_df$Prediction]
ggplot(svm_cm_df, aes(x=Prediction, y=Reference, fill=Prop)) +
geom_tile(color="white") +
geom_text(aes(label=sprintf("%.2f", Prop)), size=6, fontface="bold") +
scale_fill_gradient(low="#F5F5F5", high="#388E3C") +
labs(
title="SVM Confusion Matrix (Normalized)",
x="Predicted Class",
y="Actual Class"
) +
theme_minimal()
# ----------------- Naive Bayes Model -----------------
nb_model <- naiveBayes(stroke ~ ., data = train_smote)
nb_pred <- predict(nb_model, newdata = testData)
nb_cm <- confusionMatrix(nb_pred, testData$stroke, positive = "1")
cat("\n--- Naive Bayes ---\n")
cat("Accuracy :", round(nb_cm$overall["Accuracy"], 4), "\n")
cat("Precision:", round(nb_cm$byClass["Precision"], 4), "\n")
cat("Recall   :", round(nb_cm$byClass["Recall"], 4), "\n")
cat("F1 Score :", round(nb_cm$byClass["F1"], 4), "\n")
# Confusion Matrix Plot (normalized)
nb_cm_df <- as.data.frame(nb_cm$table)
nb_cm_df$Prop <- nb_cm_df$Freq / rowSums(nb_cm$table)[nb_cm_df$Prediction]
ggplot(nb_cm_df, aes(x=Prediction, y=Reference, fill=Prop)) +
geom_tile(color="white") +
geom_text(aes(label=sprintf("%.2f", Prop)), size=6, fontface="bold") +
scale_fill_gradient(low="#F5F5F5", high="#FF8A65") +
labs(
title="Naive Bayes Confusion Matrix (Normalized)",
x="Predicted Class",
y="Actual Class"
) +
theme_minimal()
# ----------------- PCA Visualization for SVM -----------------
X <- train_smote[, setdiff(names(train_smote), "stroke")]
y <- train_smote$stroke
pca <- prcomp(X, scale. = TRUE)
pca_df <- data.frame(
PC1 = pca$x[, 1],
PC2 = pca$x[, 2],
stroke = y
)
grid <- expand.grid(
PC1 = seq(min(pca_df$PC1), max(pca_df$PC1), length.out = 200),
PC2 = seq(min(pca_df$PC2), max(pca_df$PC2), length.out = 200)
)
svm_pca <- svm(
stroke ~ PC1 + PC2,
data = pca_df,
kernel = "radial",
cost = 1,
gamma = 0.5
)
grid$svm_pred <- predict(svm_pca, grid)
ggplot() +
geom_tile(data = grid, aes(x = PC1, y = PC2, fill = svm_pred), alpha = 0.25) +
geom_point(data = pca_df, aes(x = PC1, y = PC2, color = stroke), size = 1.5) +
labs(
title = "2D PCA Projection with SVM Decision Boundary",
subtitle = "First two principal components",
x = "PC1",
y = "PC2"
) +
scale_fill_manual(values = c("0" = "#BBDEFB", "1" = "#FFCDD2")) +
scale_color_manual(values = c("0" = "blue", "1" = "red")) +
theme_minimal(base_size = 13) +
guides(fill = "none")
# ----------------- Feature Densities for Naive Bayes -----------------
plot_df <- train_smote %>%
select(stroke, age, avg_glucose_level, bmi) %>%
pivot_longer(
cols = c(age, avg_glucose_level, bmi),
names_to = "feature",
values_to = "value"
)
ggplot(plot_df, aes(x = value, fill = stroke)) +
geom_density(alpha = 0.4) +
facet_wrap(~ feature, scales = "free") +
labs(
title = "Class-Conditional Feature Distributions (Naive Bayes)",
subtitle = "Likelihoods P(feature | stroke)",
x = "Feature value",
y = "Density",
fill = "Stroke"
) +
scale_fill_manual(values = c("0" = "#90CAF9", "1" = "#EF9A9A")) +
theme_minimal(base_size = 13)
###############################################################################
#                          1st Model â€“ Decision Tree
###############################################################################
library(rpart)
library(rpart.plot)
library(smotefamily)
library(caret)
library(ggplot2)
# Load dataset
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
# Explicit feature list (prevents mismatch bugs)
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Stratified 80/20 split
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
# Categorical columns
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Prepare numeric data for SMOTE
train_num <- trainData
for (col in categorical_cols) {
train_num[[col]] <- as.numeric(as.factor(train_num[[col]]))
}
# Target to numeric for SMOTE
train_num$stroke <- as.numeric(as.character(train_num$stroke))
# Apply SMOTE
set.seed(123)
smote_out <- SMOTE(
X = train_num[, -which(names(train_num) == "stroke")],
target = train_num$stroke,
K = 5,
dup_size = 3
)
# Rebuild SMOTE-balanced dataset
train_smote <- smote_out$data
train_smote$stroke <- as.factor(train_smote$class)
train_smote$class <- NULL
# Cost-sensitive loss matrix (penalize FN heavily)
loss_matrix <- matrix(
c(0, 1,
10, 0),
nrow = 2,
byrow = TRUE
)
colnames(loss_matrix) <- rownames(loss_matrix) <- levels(train_smote$stroke)
# Train Decision Tree
tree_model <- rpart(
stroke ~ .,
data = train_smote,
method = "class",
parms = list(loss = loss_matrix),
control = rpart.control(
cp = 0.001,
minsplit = 50,
minbucket = 10,
maxdepth = 10
)
)
# Plot tree
rpart.plot(
tree_model,
type = 3,
extra = 101,
fallen.leaves = TRUE,
main = "Decision Tree for Stroke Prediction"
)
# Encode test data using training levels (CRITICAL FIX)
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]],
levels = levels(as.factor(trainData[[col]])))
)
}
# Predictions
pred_dt <- predict(tree_model, newdata = testData, type = "class")
# Evaluation
cm_dt <- confusionMatrix(pred_dt, testData$stroke, positive = "1")
print(cm_dt)
cm_df <- as.data.frame(cm_dt$table)
ggplot(cm_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 6, fontface = "bold") +
scale_fill_gradient(low = "#F5F5F5", high = "#1976D2") +
labs(
title = "Confusion Matrix â€“ Decision Tree",
subtitle = "Cost-Sensitive Learning with SMOTE",
x = "Predicted Class",
y = "Actual Class"
) +
theme_minimal()
# Metrics
cat("Accuracy :", round(cm_dt$overall["Accuracy"], 4), "\n")
cat("Precision:", round(cm_dt$byClass["Precision"], 4), "\n")
cat("Recall   :", round(cm_dt$byClass["Recall"], 4), "\n")
cat("F1 Score :", round(cm_dt$byClass["F1"], 4), "\n")
###############################################################################
#                 Evaluation â€“ ROC & AUC Comparison
###############################################################################
library(pROC)
library(caret)
# ----------------- Load trained models -----------------
source("Descision_Tree.R")
source("Logistic_Regression.R")
source("Random_forest.R")
source("svm_naive_models.R")  # contains both SVM and Naive Bayes
# ----------------- Load and prepare data -----------------
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Same train-test split for all models
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Encode categorical columns in test set using training levels
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = levels(as.factor(trainData[[col]])))
)
}
# Ensure target factor levels match
testData$stroke <- factor(testData$stroke, levels = c("0", "1"))
# ----------------- Predicted probabilities -----------------
prob_log <- predict(log_model, testData, type = "response")
prob_dt  <- predict(tree_model, testData, type = "prob")[,2]
prob_rf  <- predict(rf_model, testData, type = "prob")[,2]
prob_nb  <- predict(nb_model, testData, type = "raw")[,2]
prob_svm <- attr(predict(svm_model, testData, probability = TRUE), "probabilities")[,2]
# ----------------- ROC Curves -----------------
roc_log <- roc(testData$stroke, prob_log)
roc_dt  <- roc(testData$stroke, prob_dt)
roc_rf  <- roc(testData$stroke, prob_rf)
roc_nb  <- roc(testData$stroke, prob_nb)
roc_svm <- roc(testData$stroke, prob_svm)
# ----------------- Open new device to show plot -----------------
dev.new()  # ðŸ”‘ this makes sure the plot window opens
plot(roc_log, col="blue", lwd=2, main="ROC Curve Comparison")
plot(roc_dt, col="red", lwd=2, add=TRUE)
plot(roc_rf, col="green", lwd=2, add=TRUE)
plot(roc_nb, col="purple", lwd=2, add=TRUE)
plot(roc_svm, col="orange", lwd=2, add=TRUE)
legend(
"bottomright",
legend = c(
"Logistic Regression",
"Decision Tree",
"Random Forest",
"Naive Bayes",
"SVM"
),
col = c("blue","red","green","purple","orange"),
lwd = 2
)
# ----------------- AUC Table -----------------
auc_results <- data.frame(
Model = c(
"Logistic Regression",
"Decision Tree",
"Random Forest",
"Naive Bayes",
"SVM"
),
AUC = c(
auc(roc_log),
auc(roc_dt),
auc(roc_rf),
auc(roc_nb),
auc(roc_svm)
)
)
print(auc_results)
###############################################################################
#                 Evaluation â€“ ROC & AUC Comparison
###############################################################################
library(pROC)
library(caret)
# ----------------- Load trained models -----------------
source("Descision_Tree.R")
source("Logistic_Regression.R")
source("Random_forest.R")
source("svm_naive_models.R")  # contains both SVM and Naive Bayes
# ----------------- Load and prepare data -----------------
df <- read.csv("cleaned_stroke_data.csv")
df$stroke <- as.factor(df$stroke)
selected_features <- c(
"age",
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status",
"avg_glucose_level",
"bmi",
"stroke"
)
df <- df[, selected_features]
# Same train-test split for all models
set.seed(123)
trainIndex <- createDataPartition(df$stroke, p = 0.8, list = FALSE)
trainData <- df[trainIndex, ]
testData  <- df[-trainIndex, ]
categorical_cols <- c(
"hypertension",
"heart_disease",
"ever_married",
"work_type",
"smoking_status"
)
# Encode categorical columns in test set using training levels
for (col in categorical_cols) {
testData[[col]] <- as.numeric(
factor(testData[[col]], levels = levels(as.factor(trainData[[col]])))
)
}
# Ensure target factor levels match
testData$stroke <- factor(testData$stroke, levels = c("0", "1"))
# ----------------- Predicted probabilities -----------------
prob_log <- predict(log_model, testData, type = "response")
prob_dt  <- predict(tree_model, testData, type = "prob")[,2]
prob_rf  <- predict(rf_model, testData, type = "prob")[,2]
prob_nb  <- predict(nb_model, testData, type = "raw")[,2]
prob_svm <- attr(predict(svm_model, testData, probability = TRUE), "probabilities")[,2]
# ----------------- ROC Curves -----------------
roc_log <- roc(testData$stroke, prob_log)
roc_dt  <- roc(testData$stroke, prob_dt)
roc_rf  <- roc(testData$stroke, prob_rf)
roc_nb  <- roc(testData$stroke, prob_nb)
roc_svm <- roc(testData$stroke, prob_svm)
# ----------------- Open new device to show plot -----------------
dev.new()  # ðŸ”‘ this makes sure the plot window opens
plot(roc_log, col="blue", lwd=2, main="ROC Curve Comparison")
plot(roc_dt, col="red", lwd=2, add=TRUE)
plot(roc_rf, col="green", lwd=2, add=TRUE)
plot(roc_nb, col="purple", lwd=2, add=TRUE)
plot(roc_svm, col="orange", lwd=2, add=TRUE)
legend(
"bottomright",
legend = c(
"Logistic Regression",
"Decision Tree",
"Random Forest",
"Naive Bayes",
"SVM"
),
col = c("blue","red","green","purple","orange"),
lwd = 2
)
# ----------------- AUC Table -----------------
auc_results <- data.frame(
Model = c(
"Logistic Regression",
"Decision Tree",
"Random Forest",
"Naive Bayes",
"SVM"
),
AUC = c(
auc(roc_log),
auc(roc_dt),
auc(roc_rf),
auc(roc_nb),
auc(roc_svm)
)
)
print(auc_results)
